{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2t2ArzYsg3CnSh2svYQTW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IVPITER080306/Dash_Lab_repository_f20241403/blob/main/04_v2_Double_Buffering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xObDc18ST9I",
        "outputId": "47aece84-079e-43e3-ec65-81e875c31098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting v2_double_buffering.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile v2_double_buffering.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define TILE_WIDTH 32\n",
        "// We pad the column dimension of shared memory tiles.\n",
        "#define TILE_WIDTH_PADDED (TILE_WIDTH + 1)\n",
        "\n",
        "// Each thread computes a 4x4 tile of C\n",
        "#define REG_TILE_ROWS 4\n",
        "#define REG_TILE_COLS 4\n",
        "\n",
        "// Block dimensions: 8x8 threads\n",
        "\n",
        "#define BLOCK_THREADS_X 8\n",
        "#define BLOCK_THREADS_Y 8\n",
        "// Total 64 threads per block\n",
        "\n",
        "__global__ void tiling_double_buffer_4x4(float *a, float *b, float *c, int m, int n, int k)\n",
        "{\n",
        "\n",
        "    __shared__ float tile_a[2][TILE_WIDTH][TILE_WIDTH_PADDED];\n",
        "    __shared__ float tile_b[2][TILE_WIDTH][TILE_WIDTH_PADDED];\n",
        "\n",
        "    // This tile is used *after* the main loop to coalesce writes.\n",
        "    // It does not need padding as it's written/read with 1D coalesced logic.\n",
        "    __shared__ float s_c[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "\n",
        "    // New 4x4 array (16 elements) to hold intermediate sums in registers.\n",
        "    float sum[REG_TILE_ROWS][REG_TILE_COLS];\n",
        "    for (int r = 0; r < REG_TILE_ROWS; r++) {\n",
        "        for (int c_ = 0; c_ < REG_TILE_COLS; c_++) {\n",
        "            sum[r][c_] = 0.0f;\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    int thread_idx_1d = threadIdx.y * blockDim.x + threadIdx.x; // 0-63\n",
        "    int block_size = blockDim.x * blockDim.y; // 64\n",
        "\n",
        "    int a_start_row_global = blockIdx.y * TILE_WIDTH;\n",
        "    int b_start_col_global = blockIdx.x * TILE_WIDTH;\n",
        "\n",
        "    int num_tiles = (k + TILE_WIDTH - 1) / TILE_WIDTH;\n",
        "\n",
        "\n",
        "    // This is the fast 1D load. Each thread loads 16 elements.\n",
        "    {\n",
        "        int t = 0;\n",
        "        int a_start_col = t * TILE_WIDTH;\n",
        "        int b_start_row = t * TILE_WIDTH;\n",
        "\n",
        "        for (int i = 0; i < 16; i++) { // 16 elements per thread (16 * 64 = 1024)\n",
        "            int tile_idx = thread_idx_1d + i * block_size;\n",
        "            int load_row = tile_idx / TILE_WIDTH; // Row within the 32x32 tile\n",
        "            int load_col = tile_idx % TILE_WIDTH; // Col within the 32x32 tile\n",
        "\n",
        "            int global_row_a = a_start_row_global + load_row;\n",
        "            int global_col_a = a_start_col + load_col;\n",
        "            int global_row_b = b_start_row + load_row;\n",
        "            int global_col_b = b_start_col_global + load_col;\n",
        "\n",
        "            // Write to shared memory using the *padded* column dimension\n",
        "            if (global_row_a < m && global_col_a < k)\n",
        "                tile_a[0][load_row][load_col] = a[global_row_a * k + global_col_a];\n",
        "            else\n",
        "                tile_a[0][load_row][load_col] = 0.0f;\n",
        "\n",
        "            if (global_row_b < k && global_col_b < n)\n",
        "                tile_b[0][load_row][load_col] = b[global_row_b * n + global_col_b];\n",
        "            else\n",
        "                tile_b[0][load_row][load_col] = 0.0f;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // Wait for the first tile (t=0) to be loaded\n",
        "\n",
        "\n",
        "    // This loop is now branch-free. It runs (num_tiles - 1) times.\n",
        "    for (int t = 0; t < num_tiles - 1; t++) {\n",
        "\n",
        "        int current_buffer = t % 2;\n",
        "        int next_buffer = (t + 1) % 2;\n",
        "\n",
        "\n",
        "        // This logic is now *unconditional*\n",
        "        {\n",
        "            int next_tile_k_a = (t + 1) * TILE_WIDTH; // Col for A\n",
        "            int next_tile_k_b = (t + 1) * TILE_WIDTH; // Row for B\n",
        "\n",
        "            for (int i = 0; i < 16; i++) { // 16 elements per thread\n",
        "                int tile_idx = thread_idx_1d + i * block_size;\n",
        "                int load_row = tile_idx / TILE_WIDTH;\n",
        "                int load_col = tile_idx % TILE_WIDTH;\n",
        "\n",
        "                int global_row_a = a_start_row_global + load_row;\n",
        "                int global_col_a = next_tile_k_a + load_col;\n",
        "                int global_row_b = next_tile_k_b + load_row;\n",
        "                int global_col_b = b_start_col_global + load_col;\n",
        "\n",
        "                // Load tile_a[next_buffer]\n",
        "                if (global_row_a < m && global_col_a < k)\n",
        "                    tile_a[next_buffer][load_row][load_col] = a[global_row_a * k + global_col_a];\n",
        "                else\n",
        "                    tile_a[next_buffer][load_row][load_col] = 0.0f;\n",
        "\n",
        "                // Load tile_b[next_buffer]\n",
        "                if (global_row_b < k && global_col_b < n)\n",
        "                    tile_b[next_buffer][load_row][load_col] = b[global_row_b * n + global_col_b];\n",
        "                else\n",
        "                    tile_b[next_buffer][load_row][load_col] = 0.0f;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        #pragma unroll\n",
        "        for (int i = 0; i < TILE_WIDTH; i++) {\n",
        "\n",
        "            // Load a column-vector (4 elements) from tile_a into registers\n",
        "            // Accessing [i] on the padded dimension is now bank-conflict-free.\n",
        "            float a_reg[REG_TILE_ROWS];\n",
        "            for (int r = 0; r < REG_TILE_ROWS; r++) {\n",
        "                a_reg[r] = tile_a[current_buffer][threadIdx.y * REG_TILE_ROWS + r][i];\n",
        "            }\n",
        "\n",
        "            // Load a row-vector (4 elements) from tile_b into registers\n",
        "            float b_reg[REG_TILE_COLS];\n",
        "            for (int c_ = 0; c_ < REG_TILE_COLS; c_++) {\n",
        "                int b_col_idx = threadIdx.x * REG_TILE_COLS + c_;\n",
        "                b_reg[c_] = tile_b[current_buffer][i][b_col_idx];\n",
        "            }\n",
        "\n",
        "            // Perform the 4x4 outer-product and accumulate\n",
        "            for (int r = 0; r < REG_TILE_ROWS; r++) {\n",
        "                for (int c_ = 0; c_ < REG_TILE_COLS; c_++) {\n",
        "                    sum[r][c_] += a_reg[r] * b_reg[c_];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "        //Wait for compute on t AND load on t+1 to finish\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "\n",
        "    // We've finished loading, now we just compute on the last tile that was\n",
        "    // loaded in the final iteration of the loop above.\n",
        "    int last_buffer = (num_tiles - 1) % 2;\n",
        "    #pragma unroll\n",
        "    for (int i = 0; i < TILE_WIDTH; i++) {\n",
        "\n",
        "        float a_reg[REG_TILE_ROWS];\n",
        "        for (int r = 0; r < REG_TILE_ROWS; r++) {\n",
        "            a_reg[r] = tile_a[last_buffer][threadIdx.y * REG_TILE_ROWS + r][i];\n",
        "        }\n",
        "\n",
        "        float b_reg[REG_TILE_COLS];\n",
        "        for (int c_ = 0; c_ < REG_TILE_COLS; c_++) {\n",
        "            int b_col_idx = threadIdx.x * REG_TILE_COLS + c_;\n",
        "            b_reg[c_] = tile_b[last_buffer][i][b_col_idx];\n",
        "        }\n",
        "\n",
        "        for (int r = 0; r < REG_TILE_ROWS; r++) {\n",
        "            for (int c_ = 0; c_ < REG_TILE_COLS; c_++) {\n",
        "                sum[r][c_] += a_reg[r] * b_reg[c_];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // Step 1: Write results from 4x4 registers (sum) to shared memory (s_c)\n",
        "    // This write uses the *compute-mapping* (uncoalesced)\n",
        "    for (int r = 0; r < REG_TILE_ROWS; r++) {\n",
        "        for (int c_ = 0; c_ < REG_TILE_COLS; c_++) {\n",
        "            int local_row = threadIdx.y * REG_TILE_ROWS + r;\n",
        "            int local_col = threadIdx.x * REG_TILE_COLS + c_;\n",
        "            // We use the un-padded s_c tile here.\n",
        "            s_c[local_row][local_col] = sum[r][c_];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // Wait for all sums to be in shared memory\n",
        "\n",
        "    // Step 2: Read from shared memory and write to global memory (c)\n",
        "    // This write uses the 1D *coalesced-mapping*\n",
        "    for (int i = 0; i < 16; i++) { // Each thread writes 16 elements\n",
        "        int tile_idx = thread_idx_1d + i * block_size;\n",
        "        int s_row = tile_idx / TILE_WIDTH; // Row in shared mem\n",
        "        int s_col = tile_idx % TILE_WIDTH; // Col in shared mem\n",
        "\n",
        "        int g_row = blockIdx.y * TILE_WIDTH + s_row;\n",
        "        int g_col = blockIdx.x * TILE_WIDTH + s_col;\n",
        "\n",
        "        if (g_row < m && g_col < n) {\n",
        "            c[g_row * n + g_col] = s_c[s_row][s_col];\n",
        "        }\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "void init_matrix(float *mat, int rows, int cols) {\n",
        "    for (int i = 0; i < rows * cols; i++) {\n",
        "        mat[i] = (float)rand() / RAND_MAX;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int M = 1024;\n",
        "    const int K = 1024;\n",
        "    const int N = 1024;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    float *h_a, *h_b, *h_c_gpu;\n",
        "    float *d_a, *d_b, *d_c;\n",
        "    size_t size_a = M * K * sizeof(float);\n",
        "    size_t size_b = K * N * sizeof(float);\n",
        "    size_t size_c = M * N * sizeof(float);\n",
        "    h_a = (float *)malloc(size_a);\n",
        "    h_b = (float *)malloc(size_b);\n",
        "    h_c_gpu = (float *)malloc(size_c);\n",
        "    init_matrix(h_a, M, K);\n",
        "    init_matrix(h_b, K, N);\n",
        "    cudaMalloc((void **)&d_a, size_a);\n",
        "    cudaMalloc((void **)&d_b, size_b);\n",
        "    cudaMalloc((void **)&d_c, size_c);\n",
        "    cudaMemcpy(d_a, h_a, size_a, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, size_b, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 block(BLOCK_THREADS_X, BLOCK_THREADS_Y); // (8, 8)\n",
        "    // Grid calculation is correct.\n",
        "    dim3 grid((N + TILE_WIDTH - 1) / TILE_WIDTH, (M + TILE_WIDTH - 1) / TILE_WIDTH);\n",
        "\n",
        "    // Warm-up run\n",
        "    tiling_double_buffer_4x4<<<grid, block>>>(d_a, d_b, d_c, M, N, K);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "    int iterations = 100;\n",
        "    float total_time = 0.0f;\n",
        "\n",
        "    for (int i = 0; i < iterations; i++) {\n",
        "\n",
        "        cudaEventRecord(start);\n",
        "\n",
        "\n",
        "        tiling_double_buffer_4x4<<<grid, block>>>(d_a, d_b, d_c, M, N, K);\n",
        "\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "\n",
        "        float iter_time;\n",
        "        cudaEventElapsedTime(&iter_time, start, stop);\n",
        "        total_time += iter_time;\n",
        "    }\n",
        "    printf(\"Average kernel execution time (Max Optimized): %f ms\\n\", total_time / iterations);\n",
        "    printf(\"Average GFLOPS: %f\\n\", 2.0f * N * M * K / (total_time / iterations) / 1e6);\n",
        "\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "    cudaMemcpy(h_c_gpu, d_c, size_c, cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c_gpu);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 v2_double_buffering.cu -o v2db\n",
        "!./v2db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkykmPkPSddi",
        "outputId": "9acc3089-2ba2-4dba-c1b1-7d47158d563d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average kernel execution time (Max Optimized): 2.009183 ms\n",
            "Average GFLOPS: 1068.834304\n"
          ]
        }
      ]
    }
  ]
}