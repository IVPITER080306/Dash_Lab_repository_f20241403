{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFpMxQWcGOQoY3X67o494+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IVPITER080306/Dash_Lab_repository_f20241403/blob/main/03_Register_Blocking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEfImOGnT26d",
        "outputId": "33c6a34d-7219-4b05-fea7-485a79eae907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tiling_reg_block.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile tiling_reg_block.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define TILE_WIDTH 32\n",
        "\n",
        "#define REG_TILE_WIDTH 4 // Each thread computes 4 elements in a row\n",
        "\n",
        "\n",
        "#define BLOCK_COLS 8 // Number of columns per block\n",
        "#define BLOCK_ROWS 32 // Number of rows per block\n",
        "//These blocks of 8x32 threads will cover a tile of 32x32 elements in the output matrix C\n",
        "//Registers of length 4 are used to hold intermediate sums for 4 elements in a row of C\n",
        "\n",
        "//The size of shared memory tiles is TILE_WIDTH x TILE_WIDTH (32X32)\n",
        "//The size of blocks is BLOCK_ROWS x BLOCK_COLS (8X32)\n",
        "//Each thread computes REG_TILE_WIDTH elements in a row (4 elements)\n",
        "\n",
        "\n",
        "//The reduction in shared memory accesses is achieved by having each thread load multiple elements from global memory into registers and then store them into shared memory.\n",
        "\n",
        "__global__ void tiling_reg_block(float *a, float *b, float *c, int m, int n, int k)\n",
        "{\n",
        "    __shared__ float tile_a[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ float tile_b[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    int row = blockIdx.y * TILE_WIDTH + threadIdx.y;\n",
        "\n",
        "    // Column indexing adjusted for REG_TILE_WIDTH.\n",
        "    // col is the starting column for the 4 elements this thread will compute.\n",
        "    // int col = blockIdx.x * TILE_WIDTH + threadIdx.x * REG_TILE_WIDTH; // <-- REMOVED, UNUSED\n",
        "\n",
        "\n",
        "    // Array to hold intermediate sums for 4 elements in a row\n",
        "    float sum[REG_TILE_WIDTH];\n",
        "    for (int i = 0; i < REG_TILE_WIDTH; i++) {\n",
        "        sum[i] = 0.0f;\n",
        "    }\n",
        "\n",
        "    for (int t = 0; t < (k + TILE_WIDTH - 1) / TILE_WIDTH; t++) {\n",
        "\n",
        "\n",
        "        // We have a (8, 32) block. We loop 4 times (TILE_WIDTH / BLOCK_COLS = 32/8 = 4).\n",
        "        // In each loop, all 8 threads load one column, ensuring\n",
        "        // threadIdx.x maps to consecutive memory addresses.\n",
        "        for (int j = 0; j < TILE_WIDTH; j += BLOCK_COLS) { // j = 0, 8, 16, 24\n",
        "\n",
        "            // Shared memory indices\n",
        "            int s_row = threadIdx.y;\n",
        "            int s_col = threadIdx.x + j;\n",
        "\n",
        "\n",
        "            int g_row_a = row; // Use the global row for this thread\n",
        "            int g_col_a = t * TILE_WIDTH + s_col;\n",
        "\n",
        "            if (g_row_a < m && g_col_a < k) {\n",
        "                tile_a[s_row][s_col] = a[g_row_a * k + g_col_a];\n",
        "            } else {\n",
        "                tile_a[s_row][s_col] = 0.0f;\n",
        "            }\n",
        "\n",
        "\n",
        "            int g_row_b = t * TILE_WIDTH + s_row;\n",
        "            int g_col_b = blockIdx.x * TILE_WIDTH + s_col;\n",
        "\n",
        "            if (g_row_b < k && g_col_b < n) {\n",
        "                tile_b[s_row][s_col] = b[g_row_b * n + g_col_b];\n",
        "            } else {\n",
        "                tile_b[s_row][s_col] = 0.0f;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        __syncthreads(); // Ensure all threads have loaded their elements\n",
        "\n",
        "\n",
        "        for (int i = 0; i < TILE_WIDTH; i++) {\n",
        "            float a_reg = tile_a[threadIdx.y][i]; // Load A element from shared memory\n",
        "\n",
        "            for (int j = 0; j < REG_TILE_WIDTH; j++) {\n",
        "                // Get the sequential column index this thread is responsible for\n",
        "                int b_col_idx = threadIdx.x * REG_TILE_WIDTH + j;\n",
        "                float b_reg = tile_b[i][b_col_idx]; // Load B element from shared memory\n",
        "                sum[j] += a_reg * b_reg; // Accumulate the product\n",
        "            }\n",
        "        }\n",
        "        __syncthreads(); // Ensure all threads have completed computation\n",
        "    }\n",
        "\n",
        "\n",
        "    // The computation stored results in registers in a strided layout.\n",
        "    // To write coalesced, we must first write these register values\n",
        "    // to shared memory, then read them back in a coalesced pattern.\n",
        "\n",
        "    // Step 1: Write register sums to shared memory (strided write to shared)\n",
        "    // We can reuse tile_a since the computation is done.\n",
        "    for (int j = 0; j < REG_TILE_WIDTH; j++) {\n",
        "        int s_col = threadIdx.x * REG_TILE_WIDTH + j;\n",
        "        tile_a[threadIdx.y][s_col] = sum[j];\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // Ensure all sums are written to shared memory\n",
        "\n",
        "\n",
        "    // This pattern is identical to the coalesced loading pattern.\n",
        "    for (int j = 0; j < TILE_WIDTH; j += BLOCK_COLS) { // j = 0, 8, 16, 24\n",
        "\n",
        "        int s_row = threadIdx.y;\n",
        "        int s_col = threadIdx.x + j;\n",
        "\n",
        "        int g_row = blockIdx.y * TILE_WIDTH + s_row;\n",
        "        int g_col = blockIdx.x * TILE_WIDTH + s_col;\n",
        "\n",
        "        if (g_row < m && g_col < n) {\n",
        "            c[g_row * n + g_col] = tile_a[s_row][s_col];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "void init_matrix(float *mat, int rows, int cols) {\n",
        "    for (int i = 0; i < rows * cols; i++) {\n",
        "        mat[i] = (float)rand() / RAND_MAX;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int M = 1024;\n",
        "    const int K = 1024;\n",
        "    const int N = 1024;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    float *h_a, *h_b, *h_c_gpu;\n",
        "    float *d_a, *d_b, *d_c;\n",
        "    size_t size_a = M * K * sizeof(float);\n",
        "    size_t size_b = K * N * sizeof(float);\n",
        "    size_t size_c = M * N * sizeof(float);\n",
        "    h_a = (float *)malloc(size_a);\n",
        "    h_b = (float *)malloc(size_b);\n",
        "    h_c_gpu = (float *)malloc(size_c);\n",
        "    init_matrix(h_a, M, K);\n",
        "    init_matrix(h_b, K, N);\n",
        "    cudaMalloc((void **)&d_a, size_a);\n",
        "    cudaMalloc((void **)&d_b, size_b);\n",
        "    cudaMalloc((void **)&d_c, size_c);\n",
        "    cudaMemcpy(d_a, h_a, size_a, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, size_b, cudaMemcpyHostToDevice);\n",
        "    dim3 block(BLOCK_COLS, BLOCK_ROWS); // (8X32)\n",
        "    dim3 grid((N + TILE_WIDTH - 1) / TILE_WIDTH, (M + TILE_WIDTH - 1) / TILE_WIDTH);\n",
        "    tiling_reg_block<<<grid, block>>>(d_a, d_b, d_c, M, N, K);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "    int iterations = 100;\n",
        "    float total_time = 0.0f;\n",
        "\n",
        "    for (int i = 0; i < iterations; i++) {\n",
        "\n",
        "        cudaEventRecord(start);\n",
        "\n",
        "\n",
        "        tiling_reg_block<<<grid, block>>>(d_a, d_b, d_c, M, N, K);\n",
        "\n",
        "\n",
        "        cudaEventRecord(stop);\n",
        "\n",
        "\n",
        "        cudaEventSynchronize(stop);\n",
        "\n",
        "\n",
        "        float iter_time;\n",
        "        cudaEventElapsedTime(&iter_time, start, stop);\n",
        "        total_time += iter_time;\n",
        "    }\n",
        "    printf(\"Average kernel execution time: %f ms\\n\", total_time / iterations);\n",
        "    printf(\"Average GFLOPS: %f\\n\", 2.0f * N * M * K / (total_time / iterations) / 1e6);\n",
        "\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "    cudaMemcpy(h_c_gpu, d_c, size_c, cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c_gpu);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75  tiling_reg_block.cu -o trb\n",
        "!./trb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEujL9Q-UHYL",
        "outputId": "c6910617-2001-448e-d5de-0c5a98bf8280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average kernel execution time: 3.487198 ms\n",
            "Average GFLOPS: 615.819328\n"
          ]
        }
      ]
    }
  ]
}
