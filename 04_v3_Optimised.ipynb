{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNlXgQHpef0cJIaQO56d5fU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IVPITER080306/Dash_Lab_repository_f20241403/blob/main/04_v3_Optimised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po19Oknwn5yb",
        "outputId": "32d5996e-565b-463c-b000-00c4c2234b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting v3_optimised.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile v3_optimised.cu\n",
        "    #include <stdio.h>\n",
        "    #include <cuda_runtime.h>\n",
        "    #include <stdlib.h>\n",
        "\n",
        "    #define TILE_WIDTH 32\n",
        "\n",
        "    #define REG_TILE_ROWS 4 // Each thread computes 4 elements in a row\n",
        "    #define REG_TILE_COLS 4 // Each thread computes 4 elements in a columne\n",
        "\n",
        "\n",
        "    #define BLOCK_COLS 8 // Number of columns per block (8 threads)\n",
        "    #define BLOCK_ROWS 8 // Number of rows per block (8 threads)\n",
        "    // 8x8 threads * (4x4 elements/thread) = 32x32 elements/block\n",
        "\n",
        "    __global__ void tiling_reg_block(float *a, float *b, float *c, int m, int n, int k)\n",
        "    {\n",
        "        __shared__ float tile_a[TILE_WIDTH][TILE_WIDTH];\n",
        "        __shared__ float tile_b[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "        // Row indexing adjusted for REG_TILE_ROWS.\n",
        "        // row is the starting row for the 4x4 elements this thread will compute.\n",
        "        int row = blockIdx.y * TILE_WIDTH  + threadIdx.y * REG_TILE_ROWS;\n",
        "\n",
        "        // Column indexing adjusted for REG_TILE_COLS.\n",
        "        // col is the starting column for the 4x4 elements this thread will compute.\n",
        "        int col = blockIdx.x * TILE_WIDTH + threadIdx.x * REG_TILE_COLS;\n",
        "\n",
        "\n",
        "        // Array to hold intermediate sums for 4X4 elements\n",
        "        // Initialize all sums to 0.0f\n",
        "        float sums[REG_TILE_ROWS][REG_TILE_COLS];\n",
        "        for (int i = 0; i < REG_TILE_ROWS; i++) {\n",
        "            for (int j = 0; j < REG_TILE_COLS; j++) {\n",
        "                sums[i][j] = 0.0f;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Loop over the tiles of A and B\n",
        "        for (int t = 0; t < (k + TILE_WIDTH - 1) / TILE_WIDTH; t++) {\n",
        "\n",
        "\n",
        "            // This pattern ensures coalesced reads from global memory.\n",
        "            // Each thread in the 8x8 block loads a 4x4 portion of the 32x32 tile.\n",
        "            // We do this by iterating 4 times (32/8=4) in each dimension.\n",
        "            for (int i = 0; i < TILE_WIDTH; i += BLOCK_ROWS) { // i = 0, 8, 16, 24\n",
        "                for (int j = 0; j < TILE_WIDTH; j += BLOCK_COLS) { // j = 0, 8, 16, 24\n",
        "\n",
        "                    // Shared memory indices\n",
        "                    int s_row = threadIdx.y + i;\n",
        "                    int s_col = threadIdx.x + j;\n",
        "\n",
        "                    // Global indices for A\n",
        "                    int g_row_a = blockIdx.y * TILE_WIDTH + s_row;\n",
        "                    int g_col_a = t * TILE_WIDTH + s_col;\n",
        "\n",
        "                    // Coalesced Read: consecutive threadIdx.x access consecutive g_col_a\n",
        "                    if (g_row_a < m && g_col_a < k) {\n",
        "                        tile_a[s_row][s_col] = a[g_row_a * k + g_col_a];\n",
        "                    } else {\n",
        "                        tile_a[s_row][s_col] = 0.0f;\n",
        "                    }\n",
        "\n",
        "                    // Global indices for B\n",
        "                    int g_row_b = t * TILE_WIDTH + s_row;\n",
        "                    int g_col_b = blockIdx.x * TILE_WIDTH + s_col;\n",
        "\n",
        "                    // Coalesced Read: consecutive threadIdx.x access consecutive g_col_b\n",
        "                    if (g_row_b < k && g_col_b < n) {\n",
        "                        tile_b[s_row][s_col] = b[g_row_b * n + g_col_b];\n",
        "                    } else {\n",
        "                        tile_b[s_row][s_col] = 0.0f;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "            __syncthreads();\n",
        "\n",
        "            // Compute partial sums\n",
        "            // This loop was correct in the original code.\n",
        "            // Each thread computes its 4x4 sums matrix.\n",
        "            for (int p = 0; p < TILE_WIDTH; p++) {\n",
        "                for (int i = 0; i < REG_TILE_ROWS; i++) {\n",
        "                    for (int j = 0; j < REG_TILE_COLS; j++) {\n",
        "                        sums[i][j] += tile_a[threadIdx.y * REG_TILE_ROWS + i][p] * tile_b[p][threadIdx.x * REG_TILE_COLS + j];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "            __syncthreads();\n",
        "        }\n",
        "\n",
        "\n",
        "        // Write results from registers to global memory\n",
        "        // This fixes the race condition by using the 'row' and 'col'\n",
        "        // variables defined at the top of the kernel.\n",
        "        for (int i = 0; i < REG_TILE_ROWS; i++) {\n",
        "            for (int j = 0; j < REG_TILE_COLS; j++) {\n",
        "\n",
        "                int final_row = row + i; // row is this thread's starting row\n",
        "                int final_col = col + j; // col is this thread's starting col\n",
        "\n",
        "                if (final_row < m && final_col < n) {\n",
        "                    c[final_row * n + final_col] = sums[i][j];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    void init_matrix(float *mat, int rows, int cols) {\n",
        "        for (int i = 0; i < rows * cols; i++) {\n",
        "            mat[i] = (float)rand() / RAND_MAX;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int main()\n",
        "    {\n",
        "        const int M = 1024;\n",
        "        const int K = 1024;\n",
        "        const int N = 1024;\n",
        "\n",
        "        cudaEvent_t start, stop;\n",
        "        cudaEventCreate(&start);\n",
        "        cudaEventCreate(&stop);\n",
        "\n",
        "        float *h_a, *h_b, *h_c_gpu;\n",
        "        float *d_a, *d_b, *d_c;\n",
        "        size_t size_a = M * K * sizeof(float);\n",
        "        size_t size_b = K * N * sizeof(float);\n",
        "        size_t size_c = M * N * sizeof(float);\n",
        "        h_a = (float *)malloc(size_a);\n",
        "        h_b = (float *)malloc(size_b);\n",
        "        h_c_gpu = (float *)malloc(size_c);\n",
        "        init_matrix(h_a, M, K);\n",
        "        init_matrix(h_b, K, N);\n",
        "        cudaMalloc((void **)&d_a, size_a);\n",
        "        cudaMalloc((void **)&d_b, size_b);\n",
        "        cudaMalloc((void **)&d_c, size_c);\n",
        "        cudaMemcpy(d_a, h_a, size_a, cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_b, h_b, size_b, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "        dim3 block(BLOCK_COLS, BLOCK_ROWS);\n",
        "\n",
        "\n",
        "        dim3 grid((N + TILE_WIDTH - 1) / TILE_WIDTH, (M + TILE_WIDTH - 1) / TILE_WIDTH);\n",
        "\n",
        "\n",
        "        tiling_reg_block<<<grid, block>>>(d_a, d_b, d_c, M, N, K);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "        int iterations = 100;\n",
        "        float total_time = 0.0f;\n",
        "\n",
        "        for (int i = 0; i < iterations; i++) {\n",
        "\n",
        "            cudaEventRecord(start);\n",
        "\n",
        "            tiling_reg_block<<<grid, block>>>(d_a, d_b, d_c, M, N, K);\n",
        "\n",
        "            cudaEventRecord(stop);\n",
        "            cudaEventSynchronize(stop);\n",
        "\n",
        "            float iter_time;\n",
        "            cudaEventElapsedTime(&iter_time, start, stop);\n",
        "            total_time += iter_time;\n",
        "        }\n",
        "        printf(\"Average kernel execution time: %f ms\\n\", total_time / iterations);\n",
        "        printf(\"Average GFLOPS: %f\\n\", (2.0f * M * N * K / (total_time / iterations) / 1e6));\n",
        "\n",
        "\n",
        "        cudaEventDestroy(start);\n",
        "        cudaEventDestroy(stop);\n",
        "        cudaMemcpy(h_c_gpu, d_c, size_c, cudaMemcpyDeviceToHost);\n",
        "        cudaFree(d_a);\n",
        "        cudaFree(d_b);\n",
        "        cudaFree(d_c);\n",
        "        free(h_a);\n",
        "        free(h_b);\n",
        "        free(h_c_gpu);\n",
        "        return 0;\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 v3_optimised.cu -o v3_optimised\n",
        "!./v3_optimised"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epOCZ6PTolEe",
        "outputId": "b2b9be27-1e93-4c67-e3e7-2b1be37bd9a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average kernel execution time: 2.339632 ms\n",
            "Average GFLOPS: 917.872576\n"
          ]
        }
      ]
    }
  ]
}